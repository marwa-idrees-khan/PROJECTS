{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97326ae5-bcf6-46bf-b40d-01e3c2c4ed0d",
   "metadata": {},
   "source": [
    "***>>>Identify if two sentences are similar or not    :)***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7843bc0-319e-4556-a446-d41a634375a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ANQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk import ne_chunk\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce9302cd-0c8d-409a-b281-4f6e8478d706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your string here: hello tarfa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello tarfa\n"
     ]
    }
   ],
   "source": [
    "# taking the input sentence here\n",
    "sen1=input(\"enter your string here:\")\n",
    "print(sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef591eea-6015-4c77-af09-da836665c522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your string here: hello ruqia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ruqia\n"
     ]
    }
   ],
   "source": [
    "# taking the input sentence here\n",
    "sen2=input(\"enter your string here:\")\n",
    "print(sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab30bb9b-99e9-4e6d-8ac9-c5e18efb690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the tokenizer\n",
    "custom_sent_tokenizer1=PunktSentenceTokenizer(sen1)\n",
    "tokenized1=custom_sent_tokenizer1.tokenize(sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "12362861-3fd3-4683-ab1d-ddb75a14b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the tokenizer for 2 sentence\n",
    "custom_sent_tokenizer2=PunktSentenceTokenizer(sen2)\n",
    "tokenized2=custom_sent_tokenizer2.tokenize(sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "431f6835-f8c3-4359-a864-4931791786df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello tarfa']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a372b058-d635-4dd5-9628-9af4a8dc3ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello ruqia']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "53823434-b28e-4e0e-b9b8-5911a65a563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'NN'), ('tarfa', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized1:\n",
    "    words =nltk.word_tokenize(i)         # word_tokenize divide sentemce in words \n",
    "    tagged1=nltk.pos_tag(words)           # pos_tagged tells us about part of speech (i.e verbs , preposition etc)\n",
    "    print(tagged1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c70ee3d-3e1b-493d-8d62-b178e9727212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'NN'), ('ruqia', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized2:\n",
    "    words =nltk.word_tokenize(i)         # word_tokenize divide sentemce in words \n",
    "    tagged2=nltk.pos_tag(words)           # pos_tagged tells us about part of speech (i.e verbs , preposition etc)\n",
    "    print(tagged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98d79780-9833-4133-b9ed-41477b74f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkGram=r\"\"\"                                                                   #r is reserved word and \" is part of syntax of chunking,chunkgram--chunkgrammer\n",
    "          Chunk:\n",
    "          {<NN>}       # | represents OR \n",
    "          {<NNP>}                                                        # NN represents noun with noun similarly NNP is noun with proper noun\n",
    "          {<IN>}                                                                 #IN--preposition\n",
    "          {<VBZ>}   \n",
    "          {<DT>}#VBZ--verb, DT---determinant\n",
    "\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8be225b-a737-45cb-aa10-dd12e827d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (Chunk hello/NN) (Chunk tarfa/NN))\n",
      "hi ['hello']\n",
      "(S (Chunk hello/NN) (Chunk tarfa/NN))\n",
      "hi ['hello', 'tarfa']\n"
     ]
    }
   ],
   "source": [
    "ChunkParser = nltk.RegexpParser(ChunkGram)\n",
    "\n",
    "chunked1 = ChunkParser.parse(tagged1)\n",
    "chunks1=[]\n",
    "\n",
    "for subtree in chunked1.subtrees(filter=lambda t:t.label()=='Chunk'):    # ':' means assigning\n",
    "    chunk=\"\"\n",
    "    for leave in subtree.leaves():\n",
    "        chunk +=leave[0]+' '\n",
    "        chunks1.append(chunk.strip())                            #strip remove any comma , colon etc\n",
    "    print(chunked1)\n",
    "    print(\"hi\",chunks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1452350a-b672-4853-9ee8-e82b222e3f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (Chunk hello/NN) (Chunk ruqia/NN))\n",
      "hi ['hello']\n",
      "(S (Chunk hello/NN) (Chunk ruqia/NN))\n",
      "hi ['hello', 'ruqia']\n"
     ]
    }
   ],
   "source": [
    "ChunkParser = nltk.RegexpParser(ChunkGram)\n",
    "\n",
    "chunked2 = ChunkParser.parse(tagged2)\n",
    "chunks2=[]\n",
    "\n",
    "for subtree in chunked2.subtrees(filter=lambda t:t.label()=='Chunk'):    # ':' means assigning\n",
    "    chunk=\"\"\n",
    "    for leave in subtree.leaves():\n",
    "        chunk +=leave[0]+' '\n",
    "        chunks2.append(chunk.strip())                            #strip remove any comma , colon etc\n",
    "    print(chunked2)\n",
    "    print(\"hi\",chunks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ac4564c9-acf9-466e-bca0-574de4d4a48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "finalchunks1=[]\n",
    "finalnoun=[]\n",
    "for idx, item in enumerate(chunks1[:-1]):\n",
    "    if(item in chunks1[idx+1]):\n",
    "        finalchunks1.append(item)\n",
    "print(finalchunks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6044ac49-6841-4ec5-8767-cdbe8f9845eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final noun phrase extracted from parser ['hello', 'tarfa']\n"
     ]
    }
   ],
   "source": [
    "for i in chunks1:\n",
    "    if i not in finalchunks1:\n",
    "        finalnoun.append(i)\n",
    "print(\"final noun phrase extracted from parser\",finalnoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bfc00ef7-03fe-4943-90d1-1122d7301005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, Synset('hello.n.01')]\n"
     ]
    }
   ],
   "source": [
    "simi=[]\n",
    "for word1 in chunks1:\n",
    "    for word2 in chunks2:\n",
    "        wordfromlist1=wordnet.synsets(word1)\n",
    "        wordfromlist2=wordnet.synsets(word2)\n",
    "        if wordfromlist1 and wordfromlist2:\n",
    "            s=wordfromlist1[0].wup_similarity(wordfromlist2[0])\n",
    "            simi.append(s)\n",
    "            simi.append(wordfromlist2[0])\n",
    "\n",
    "print(simi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e160ad7c-ca78-49e3-a122-99e684d32b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m wordfroml2\u001b[38;5;241m=\u001b[39mwordnet\u001b[38;5;241m.\u001b[39msynsets(word2)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wordfroml1 \u001b[38;5;129;01mand\u001b[39;00m wordfroml2:\n\u001b[1;32m----> 7\u001b[0m     s\u001b[38;5;241m=\u001b[39m\u001b[43mwordfroml1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mwup_similarity(wordfroml2[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      8\u001b[0m     simi_1\u001b[38;5;241m.\u001b[39mappend(s)\n\u001b[0;32m      9\u001b[0m     simi_1\u001b[38;5;241m.\u001b[39mappend(wordfroml2[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "simi_1=[]\n",
    "for word1 in chunks1:\n",
    "    for word2 in chunks2:\n",
    "        wordfroml1=wordnet.synsets(word1)\n",
    "        wordfroml2=wordnet.synsets(word2)\n",
    "        if wordfroml1 and wordfroml2:\n",
    "            s=wordfroml1[1].wup_similarity(wordfroml2[1])\n",
    "            simi_1.append(s)\n",
    "            simi_1.append(wordfroml2[1])\n",
    "\n",
    "print(simi_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d812a370-1fc5-42a9-a70d-d5a1d9ff4cd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m synonyms\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m syn \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimi\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m syn\u001b[38;5;241m.\u001b[39mlemmas():\n\u001b[0;32m      5\u001b[0m         synonyms\u001b[38;5;241m.\u001b[39mappend(i\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1765\u001b[0m, in \u001b[0;36mWordNetCorpusReader.synsets\u001b[1;34m(self, lemma, pos, lang, check_exceptions)\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynsets\u001b[39m(\u001b[38;5;28mself\u001b[39m, lemma, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m, check_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load all synsets with a given lemma and part of speech tag.\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;124;03m    If no pos is specified, all synsets for all parts of speech\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m \u001b[38;5;124;03m    will be loaded.\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;124;03m    If lang is specified, all the synsets associated with the lemma name\u001b[39;00m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;124;03m    of that language will be returned.\u001b[39;00m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1765\u001b[0m     lemma \u001b[38;5;241m=\u001b[39m \u001b[43mlemma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m   1767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1768\u001b[0m         get_synset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynset_from_pos_and_offset\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "synonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(simi)[0]:\n",
    "    for i in syn.lemmas():\n",
    "        synonyms.append(i.name())\n",
    "\n",
    "print(set(synonyms))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
