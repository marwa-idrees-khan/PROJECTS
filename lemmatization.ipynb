{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7701e87-7937-4735-a75d-da6b75a49bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer      #stem means root\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51104bb-d4e3-402a-8d26-a141eb5b55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()     #wordnet---synonyms and antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45279f3c-8786-49f2-94d4-0d5268a1cd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "print(\"rocks :\",lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\",lemmatizer.lemmatize(\"corpora\"))   #corpora means dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ad4756-763b-4d4a-9555-24264e11aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better:  good\n"
     ]
    }
   ],
   "source": [
    "#a denotes adjective in \"pos\"\n",
    "print('better: ',lemmatizer.lemmatize('better',pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57b4574-585a-4a98-9fb1-e191c01521d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your word here: beds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beds\n"
     ]
    }
   ],
   "source": [
    "sen=input(\"enter your word here:\")\n",
    "print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29de7f37-cf90-4df6-a83f-cca06aaf633e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beds bed\n"
     ]
    }
   ],
   "source": [
    "print(sen, lemmatizer.lemmatize(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4fef9e-43a2-4f64-aa5b-e3f9684ae971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking:  walk\n"
     ]
    }
   ],
   "source": [
    "print('walking: ',lemmatizer.lemmatize('walking',pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0141e60a-1a0d-4904-b5ae-58564c7a4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#single word lemmatization function\n",
    "list1=['kites','babies','dogs','flying','smiling','driving','died','tried','feet' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09759015-6c2c-41f8-8524-f3b0515039da",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33f1b0bf-1194-40cc-a493-a19980fb4a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites.....>kite\n",
      "babies.....>baby\n",
      "dogs.....>dog\n",
      "flying.....>flying\n",
      "smiling.....>smiling\n",
      "driving.....>driving\n",
      "died.....>died\n",
      "tried.....>tried\n",
      "feet.....>foot\n"
     ]
    }
   ],
   "source": [
    "for words in list1:\n",
    "    print(words+\".....>\"+ wnl.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da6d0b15-b6e0-4fa8-a5e1-a42b7887cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "099d9a78-2741-4134-8950-c13ce39d9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the tokenizer\n",
    "custom_sent_tokenizer=PunktSentenceTokenizer(sen)\n",
    "tokenized=custom_sent_tokenizer.tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e9035fd-600f-4a05-af78-926ff2205a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beds']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ba2459b-a4fd-4222-9c97-c5c2bf077359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('beds', 'NNS')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ANQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ANQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "for i in tokenized:\n",
    "    words =nltk.word_tokenize(i)         # word_tokenize divide sentemce in words \n",
    "    tagged=nltk.pos_tag(words)           # pos_tagged tells us about part of speech (i.e verbs , preposition etc)\n",
    "    print(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d57eb83-82fe-453a-a283-104f4b77a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_tagger function\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):  \n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "         return wordnet.ADV\n",
    "    else:\n",
    "         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f12f7c01-cb3d-4890-a2c2-fe72649c0c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'), ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('badly', 'RB'), ('flying', 'VBG'), ('geese', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "sentence ='the cat is sitting with the bats on the striped mat under many badly flying geese'\n",
    "pos_tagged= nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8273defa-298a-49ff-a4bb-9d9d9eb715d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None), ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'), ('mat', 'n'), ('under', None), ('many', 'a'), ('badly', 'r'), ('flying', 'v'), ('geese', 'a')]\n"
     ]
    }
   ],
   "source": [
    "wordnet_tagged = list(map(lambda x:(x[0], pos_tagger(x[1])),pos_tagged))\n",
    "print(wordnet_tagged)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91a5f98f-26df-4a85-b3e4-716a5576b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat be sit with the bat on the striped mat under many badly fly geese\n"
     ]
    }
   ],
   "source": [
    "lemmatized_sentence = []\n",
    "for word,tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        #if there is no available tag , append the token as is\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:\n",
    "       #else use the tag to lemmatize the token \n",
    "       lemmatized_sentence.append(lemmatizer.lemmatize(word,tag))\n",
    "lemmatized_sentence=  \" \".join(lemmatized_sentence)\n",
    "print(lemmatized_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82cdbe45-35b0-46d9-aebf-4d869b22dc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  t h e   c a t   b e   s i t   w i t h   t h e   b a t   o n   t h e   s t r i p e d   m a t   u n d e r   m a n y   b a d l y   f l y   g e e s e'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "from functools import reduce \n",
    "\n",
    "stemmed_sen=reduce(lambda x , y:x+\" \" + ps.stem(y),lemmatized_sentence,\" \")\n",
    "stemmed_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0245c0-226c-4237-a652-ab4a4e766fdd",
   "metadata": {},
   "source": [
    "***lemmatization using TestBlob and Spacy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebf24782-8522-4310-a088-3fb4aef71a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
